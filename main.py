import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from peft import PeftModel

# ê²½ë¡œ ì„¤ì •
base_model_name = "Qwen/Qwen3-VL-4B-Instruct"
lora_adapter_path = "./qwen3_qlora_final"  # QLoRA adapterê°€ ì €ì¥ëœ ê²½ë¡œ
merged_model_path = "./qwen3_vl_merged"

print("="*60)
print("Qwen3-VL-4B-Instruct + QLoRA Adapter Merge")
print("="*60)

# 1. 4-bit ì–‘ìí™” ì„¤ì •
print(f"\n1. Setting up 4-bit quantization config...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)
print(f"   âœ“ Quantization config created")

# 2. Base VL ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)
print(f"\n2. Loading base model with 4-bit quantization: {base_model_name}")
base_model = Qwen3VLForConditionalGeneration.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
print(f"   âœ“ Base model loaded (4-bit quantized)")

# 3. Processor ë¡œë“œ
print(f"\n3. Loading processor...")
processor = AutoProcessor.from_pretrained(base_model_name)
print(f"   âœ“ Processor loaded")

# 4. QLoRA adapter ë¡œë“œ
print(f"\n4. Loading QLoRA adapter from: {lora_adapter_path}")
model = PeftModel.from_pretrained(base_model, lora_adapter_path)
print(f"   âœ“ QLoRA adapter loaded")

# 5. ëª¨ë¸ merge (dequantize í›„ merge)
print("\n5. Merging QLoRA weights into base model...")
print("   âš ï¸  Note: Merging will dequantize the model to full precision")
merged_model = model.merge_and_unload()
print(f"   âœ“ Merge completed")

# 6. Merged ëª¨ë¸ ì €ì¥
print(f"\n6. Saving merged model to: {merged_model_path}")
merged_model.save_pretrained(merged_model_path)
processor.save_pretrained(merged_model_path)
print(f"   âœ“ Model saved (full precision)")

print("\n" + "="*60)
print("Merge ì™„ë£Œ!")
print("="*60)

# 7. Merged ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì„¤ëª…)
print("\n" + "="*60)
print("Merged Model í…ŒìŠ¤íŠ¸")
print("="*60)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
print("\nPreparing inputs...")
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(merged_model.device)

# Inference
print("Generating response...")
generated_ids = merged_model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)

print("\n" + "-"*60)
print("Response:")
print("-"*60)
print(output_text[0])
print("-"*60)

# 8. ëª¨ë¸ ì •ë³´ ì¶œë ¥
print("\n" + "="*60)
print("ëª¨ë¸ ì •ë³´")
print("="*60)
print(f"Base model: {base_model_name}")
print(f"QLoRA adapter: {lora_adapter_path}")
print(f"Merged model saved to: {merged_model_path}")
print(f"Model dtype: {merged_model.dtype}")
print(f"Total parameters: {merged_model.num_parameters():,}")
print("="*60)

print("\nâœ“ ì´ì œ merged ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
print(f"  model = Qwen3VLForConditionalGeneration.from_pretrained('{merged_model_path}')")
#print(f"  processor = AutoProcessor.from_pretrained('{merged_model_path}')")
##print("\nğŸ’¡ ì°¸ê³ : Merged ëª¨ë¸ì€ full precisionì…ë‹ˆë‹¤.")
##print("   ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ë ¤ë©´ ë‹¤ì‹œ ì–‘ìí™”í•˜ì—¬ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
#print(f"  model = Qwen3VLForConditionalGeneration.from_pretrained('{merged_model_path}', quantization_config=bnb_config)")
